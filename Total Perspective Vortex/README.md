<!--HEADER-->
<h1 align="center"> 42 Outer |Â 
 <picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://cdn.simpleicons.org/42/white">
  <img alt="42" width=40 align="top" src="https://cdn.simpleicons.org/42/Black">
 </picture>
 Cursus 
<img alt="Complete" src="https://raw.githubusercontent.com/Mqxx/GitHub-Markdown/main/blockquotes/badge/dark-theme/complete.svg"> </h1>
<!--FINISH HEADER-->

## Total Perspective Vortex

### Descripion
- With this exercise we lean how to manage EEG data with MNE library. (Brain siganals)
### Environment ###
Your system should have an environment variable called: NME_DATA wher all pyshonet data can be found.

- **__Execution__**: 
  All scripts have to be executed inside the **Total perspective vortex** folder with the following configuration:
      
      python -m Code.folder.filename "[S1,S2...S109]" "[R1,R2..R14]" "classifier type"
    
    Where R = run number and S = Subject number
    
    The classifier type works only for the Mandatory.mybci can be: GAUSSIAN, GAUSSIAN_O_Vs_O, GAUSSIAN_O_Vs_R, KNN, KNN_O_Vs_O, KNN_O_Vs_R, DT, DT_O_Vs_O, DT_O_Vs_R, SVM, SVM_O_Vs_O, SVM_O_Vs_R, POLY_SVM, POLY_SVM_O_Vs_O, POLY_SVM_O_Vs_R, SIG_SVM, SIG_SVM_O_Vs_O, SIG_SVM_O_Vs_R, RF, RF_O_Vs_O, RF_O_Vs_R, LDA, LDA_O_Vs_O, LDA_O_Vs_R, LR, LR_O_Vs_O, LR_O_Vs_R.
    
    Where:
    KNN : K Neighbors 
    DT : Decission Tree
    SVM : Suport vector Machine (Linear)
    POLY_SVM : Suport Vector machine (poinomial)
    SIG_SVM : Suport Vector machine (sigmoid)
    RF : Random Forest
    LDA : Linear discriinant Analysis
    LR : Logistic Regression

    All classifieres ended with O_Vs_O, O_Vs_R make sense to classify more than 2 types of envets.
    O_Vs_O = One versus one
    O_Vs_R = One versus rest

### Folder configuration, files description: ###

- **__Code/Analysis/mybci.py__**: 

    -File to execute: mybci.py

    Example: 
    This function will fit all classifiers for files S001R03, S001R07,S001R11
      
      python -m Code.Analysis.mybci "[1]" "[3,7,11]"
      python -m Code.Analysis.mybci "[1,2]" "[3,7,11]"
      python -m Code.Analysis.mybci "[1]" "[3,5,7,9,11,13]"
      python -m Code.Analysis.mybci "[1,2]" "[3,5,7,9,11,13]"

- **__Code/Mandatory/visualize.py__**: 

    Example: This function will show some imformation for the files S001R03, S001R04
        
        python -m Code.Mandatory.visualize "[1]" "[3,4]"

- **__Code/Mandatory/mybci__**: 

    Example: This function will fit the KNN classifiers for files S001R03, S001R07,S001R11
        
        python -m Code.Mandatory.mybci "[1]" "[3,7,11]" "KNN"

- **__Code/Mandatory/mybci_predict__**: 

    Example: This function will predict the events for files S001R03 using the fit information saved with the last execution of Code/Mandatory/mybci
        
        python -m Code.Mandatory.mybci "[1]" "[3]"

- **__Code/Bonus/mybci__**: 

    Example: This function will fit using Neural network (NN) class the events for files S001R03, S001R07,S001R11
        
        python -m Code.Bonus.mybci "[1]" "[3,7,11]"

- **__Code/Bonus/mybci_std__**: 

    Example: This function will fit using the scikit NN class the events for files S001R03, S001R07,S001R11
        
        python -m Code.Bonus.mybci_std "[1]" "[3,7,11]"

- **__Code/Bonus/mybci_predict_std__**: 

    Example: This function will predict the event using the fit result given by mybci_std for files S001R03

        python -m Code.Bonus.mybci_tensor "[1]" "[3]"

- **__Code/Bonus/mybci_predict_std__**: 

    Example: This function will predict using the CNN tensorflow library class the events for files S001R03, S001R07,S001R11
        
        python -m Code.Bonus.mybci_tensor "[1]" "[3,7,11]"

- **__Code/Bonus/mybci_std__**: 

    Example: This function will fit using the RNN tensorflow class the events for files S001R03, S001R07,S001R11
        
        python -m Code.Bonus.mybci_RNN "[1]" "[3,7,11]"

### Pictures

  This is a curiose image generated by CHatGPT (Using DALEE) concerning the structure of the .fif fle.
<p>
  <img src=>
</p>

### Results
There are different algorithms for predict the EEG data.


**Results comparing each classifiers for 2 events (real)**

    python -m Code.Analysis.mybci "[1]" "[3,7,11]"

|Clasifier | Validation precision | Test Precision|
|----------|----------------------|---------------|
| GAUSSIAN | 1.00 | 0.62 |
| KNN | 0.97 | 0.52 |
| DT | 1.00 | 0.52 |
| SVM | 0.97 | 0.77 |
| POLY_SVM | 0.85 | 0.77 |
| SIG_SVM | 0.97 | 0.52 |
| RF | 1.00 | 0.42 |
| LDA | 0.97 | 0.52 |
| LR | 1.00 | 0.77 |


**Results comparing classifiers for 2 events (imaginery)**

    python -m Code.Analysis.mybci "[1]" "[4,8,12]"

|Clasifier | Validation precision | Test Precision|
|----------|----------------------|---------------|
| GAUSSIAN | 1.00 | 0.61 |
| KNN | 1.00 | 0.61 |
| DT | 1.00 | 0.60 |
| SVM | 1.00 | 0.55 |
| POLY_SVM | 1.00 | 0.59 |
| SIG_SVM | 1.00 | 0.55 |
| RF | 1.00 | 0.55 |
| LDA | 1.00 | 0.61 |
| LR | 1.00 | 0.55 |


When predicting more than two events it is necessary to include the function One versus One classifier (O_Vs_O) or One versus Rest classifier (O_Vs_R)
In some cases the result is impproved.
Allthow this improvement some predictors have an accuracy very low when we include the 2 other possible events.

**Resutls for 4 real events**

    python -m Code.Analysis.mybci "[1]" "[3,5,7,9,11,13]"

|Clasifier | Validation precision | Test Precision|
|----------|----------------------|---------------|
| GAUSSIAN | 0.81 | 0.69 |
| GAUSSIAN_O_Vs_O | 0.94 | 0.77 |
| GAUSSIAN_O_Vs_R | 0.81 | 0.69 |
| KNN | 0.82 | 0.72 |
| KNN_O_Vs_O | 0.82 | 0.77 |
| KNN_O_Vs_R | 0.84 | 0.42 |
| DT | 1.00 | 0.77 |
| DT_O_Vs_O | 1.00 | 1.00 |
| DT_O_Vs_R | 1.00 | 0.77 |
| SVM | 0.80 | 0.69 |
| SVM_O_Vs_O | 0.80 | 0.77 |
| SVM_O_Vs_R | 0.80 | 0.69 |
| POLY_SVM | 0.77 | 0.53 |
| POLY_SVM_O_Vs_O | 0.77 | 0.53 |
| POLY_SVM_O_Vs_R | 0.85 | 0.69 |
| SIG_SVM | 0.71 | 0.65 |
| SIG_SVM_O_Vs_O | 0.72 | 0.65 |
| SIG_SVM_O_Vs_R | 0.72 | 0.72 |
| RF | 1.00 | 0.77 |
| RF_O_Vs_O | 1.00 | 0.72 |
| RF_O_Vs_R | 1.00 | 0.72 |
| LDA | 0.78 | 0.69 |
| LDA_O_Vs_O | 0.80 | 0.77 |
| LDA_O_Vs_R | 0.80 | 0.69 |
| LR | 0.75 | 0.69 |
| LR_O_Vs_O | 0.82 | 0.77 |
| LR_O_Vs_R | 0.75 | 0.69 |

**Resutls for 4 real imaginery**

    python -m Code.Analysis.mybci "[1]" "[4,6,8,10,12,14]"


|Clasifier | Validation precision | Test Precision|
|----------|----------------------|---------------|
| GAUSSIAN | 0.79 | 0.23 |
| GAUSSIAN_O_Vs_O | 0.80 | 0.23 |
| GAUSSIAN_O_Vs_R | 0.79 | 0.23 |
| KNN | 0.82 | 0.36 |
| KNN_O_Vs_O | 0.80 | 0.36 |
| KNN_O_Vs_R | 0.77 | 0.63 |
| DT | 1.00 | 0.67 |
| DT_O_Vs_O | 1.00 | 0.23 |
| DT_O_Vs_R | 1.00 | 0.18 |
| SVM | 0.84 | 1.00 |
| SVM_O_Vs_O | 0.84 | 1.00 |
| SVM_O_Vs_R | 0.77 | 0.19 |
| POLY_SVM | 0.83 | 0.00 |
| POLY_SVM_O_Vs_O | 0.83 | 0.00 |
| POLY_SVM_O_Vs_R | 0.82 | 0.00 |
| SIG_SVM | 0.77 | 0.31 |
| SIG_SVM_O_Vs_O | 0.77 | 0.31 |
| SIG_SVM_O_Vs_R | 0.61 | 0.13 |
| RF | 1.00 | 0.32 |
| RF_O_Vs_O | 1.00 | 0.21 |
| RF_O_Vs_R | 1.00 | 0.32 |
| LDA | 0.81 | 0.23 |
| LDA_O_Vs_O | 0.81 | 0.32 |
| LDA_O_Vs_R | 0.75 | 0.23 |
| LR | 0.74 | 0.23 |
| LR_O_Vs_O | 0.79 | 0.23 |
| LR_O_Vs_R | 0.74 | 0.23 |

**Results with 2 subjects and 4 events**
In case we include other subjects we get worse resutls.

    python -m Code.Analysis.mybci "[1,3]" "[4,6,8,10,12,14]"

### Resources

* **[MNE Ligbary](https://mne.tools/1.8/index.html)**
* **[MEG and EEG data analysis with MNE-Python - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC3872725/)**
* **[Dimensionality reduction PCA](https://ethen8181.github.io/machine-learning/dim_reduct/PCA.html#PCA)**
* **[Introduction to EEG analysis](https://alexenge.github.io/intro-to-eeg/misc/index.html)**
* **[Understanding Dimension Reduction with Principal Component Analysis (PCA)](https://blog.paperspace.com/dimension-reduction-with-principal-component-analysis/)**
* **[Convolutional Neural Network from Scratch | Mathematics & Python Code](https://www.youtube.com/watch?v=Lakz2MoHy6o)**
* **[Convolutional Neural Network from Scratch | Mathematics & Python Code Github](https://github.com/TheIndependentCode/Neural-Network)**
